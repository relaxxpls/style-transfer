{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import ImageDataset\n",
    "from models import Generator, Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = Path(\"./datasets/17flowers\")\n",
    "RESULTS_DIR = Path(\"./results/17flowers\")\n",
    "CHECKPOINTS_DIR = Path(\"./checkpoints/17flowers\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 512\n",
    "IMG_CHANNELS = 3\n",
    "BATCH_SIZE = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ImageDataset(root=DATASET_DIR, transform=transform, unaligned=True)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG_A2B = Generator(in_channels=IMG_CHANNELS, out_channels=IMG_CHANNELS).to(device)\n",
    "netG_B2A = Generator(in_channels=IMG_CHANNELS, out_channels=IMG_CHANNELS).to(device)\n",
    "netD_A = Discriminator(in_channels=IMG_CHANNELS).to(device)\n",
    "netD_B = Discriminator(in_channels=IMG_CHANNELS).to(device)\n",
    "\n",
    "netG_A2B.load_state_dict(torch.load(CHECKPOINTS_DIR / \"netG_A2B_final.pth\"))\n",
    "netG_B2A.load_state_dict(torch.load(CHECKPOINTS_DIR / \"netG_B2A_final.pth\"))\n",
    "netD_A.load_state_dict(torch.load(CHECKPOINTS_DIR / \"netD_A_final.pth\"))\n",
    "netD_B.load_state_dict(torch.load(CHECKPOINTS_DIR / \"netD_B_final.pth\"))\n",
    "\n",
    "print(\"Successfully loaded models!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fadein_gif(images, transitions=4, save_path=RESULTS_DIR / \"recursive\" / \"fadein.gif\"):\n",
    "    frames = []\n",
    "    for i in range(len(images) - 1):\n",
    "        img1 = (images[i] * 255 / images[i].max()).astype('uint8')\n",
    "        img2 = (images[i+1] * 255 / images[i+1].max()).astype('uint8')\n",
    "\n",
    "        for i in range(0, transitions):\n",
    "            fadein = i / transitions\n",
    "            img = cv2.addWeighted(img1, 1-fadein, img2, fadein, 0)\n",
    "            img = Image.fromarray(img)\n",
    "            frames.append(img)\n",
    "\n",
    "    frames.append(Image.fromarray(img2))\n",
    "    frames[0].save(save_path, format='GIF', append_images=frames[1:], save_all=True, duration=500/transitions, loop=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_progression_A(sample_batch, passes=8):\n",
    "    assert passes > 1 and passes < 15, \"passes must be between 2 and 15\"\n",
    "\n",
    "    fig, axes = plt.subplots(BATCH_SIZE, passes, figsize=(passes*2.5, BATCH_SIZE*2.5), constrained_layout=True)\n",
    "    [ax.set_axis_off() for ax in axes.ravel()]\n",
    "    axes = list(map(list, zip(*axes)))\n",
    "    generated_imgs = [sample_batch[0].cpu().permute(1, 2, 0).clamp(0, 1).numpy()]\n",
    "\n",
    "    title = \"A\"\n",
    "    for j, axj in enumerate(axes[0]):\n",
    "        axj.imshow(sample_batch[j].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "        axj.set_title(f\"{title}\")\n",
    "\n",
    "    for i in range(1, passes):\n",
    "        if i % 2:\n",
    "            title += \"2B\"\n",
    "            sample_batch = netG_A2B(sample_batch).detach()\n",
    "            for j, axj in enumerate(axes[i]):\n",
    "                axj.imshow(sample_batch[j].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "                axj.set_title(f\"{title}\")\n",
    "\n",
    "        else:\n",
    "            title += \"2A\"\n",
    "            sample_batch = netG_B2A(sample_batch).detach()\n",
    "            for j, axj in enumerate(axes[i]):\n",
    "                axj.imshow(sample_batch[j].cpu().permute(1, 2, 0).clamp(0, 1))\n",
    "                axj.set_title(f\"{title}\")\n",
    "\n",
    "        generated_imgs.append(sample_batch[0].cpu().permute(1, 2, 0).clamp(0, 1).numpy())\n",
    "\n",
    "    fig.suptitle(f\"17flowers - {title} (Passes: {passes})\", fontsize=20)\n",
    "    fig.savefig(RESULTS_DIR / \"recursive\" / f\"passes_{passes}.jpg\", facecolor=\"w\", dpi=300)\n",
    "\n",
    "    make_fadein_gif(generated_imgs, save_path=RESULTS_DIR / \"recursive\" / f\"passes_{passes}.gif\")\n",
    "\n",
    "\n",
    "sample_batch = next(iter(dataloader))\n",
    "visualize_progression_A(sample_batch['A'].to(device), passes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
